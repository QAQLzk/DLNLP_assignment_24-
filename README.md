# README

## Introduction
This project is based on the Kaggle competition "U.S. Patent Phrase to Phrase Matching".（https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/overview）
This repository contains code for the pre-trained model “DeBERTa-v3-small” based on the Transformer architecture. The project started by exploring and visualizing the dataset. Then the data was preprocessed and categorized. After setting the training parameters, the model was trained. Its training and testing results were evaluated and visualized simultaneously, and its Pearson correlation coefficient reached 0.8353 and 0.8357, respectively.


## File Organization
- `Datasets/`: The folder of the dataset.
- `pre_saved_results/`: Contains the previous trained results. All images used in the report are from there. (Generated by jupyter notebook)
- `results/`: Empty folder, All output images will be generated here
- `functions.py`: Contains functions needs of whole project.(Converting from jupyter notebook)
- `main.py`: The main script for executing the project. 
- `code_notebook.ipynb`: Jupyter notebook file for the whole project.(You can pre-view the result here) 
- `README.md`: The README file for the project.

## Libraries Requirements
The works base on Python 3.11.6. The following are the libraries needed:
- numpy
- pandas 
- seaborn
- matplotlib
- datasets
- sentencepiece
- torch
- transformers
- scipy
- scikit-learn
Note: Some libraries may require that you also download their dependencies.

## How to Run
1. Clone the repository to local machine.
2. Install and check all required libraries 
3. Execute `python main.py` in terminal to begin training and evaluating the models. (Retraining models will take long time)
4. Alternatively, detailed training processes and outputs can be viewed or restart training in the `code_notebook.ipynb` (recommand!!)

